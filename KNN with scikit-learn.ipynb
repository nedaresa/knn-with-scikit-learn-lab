{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN with scikit-learn \n",
    "\n",
    "\n",
    "To use scikit-learn's implementation of a KNN classifier on the classic Titanic dataset from Kaggle!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "* Use KNN to make classification predictions on a real-world dataset\n",
    "* Perform a parameter search for 'k' to optimize model performance\n",
    "* Evaluate model performance and interpret results\n",
    "\n",
    "\n",
    "Start by importing the dataset, stored in the `titanic.csv` file, and cleaning it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('titanic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data\n",
    "- Remove unnecessary columns (PassengerId, Name, Ticket, and Cabin).\n",
    "- Convert Sex to a binary encoding, where female is 0 and male is 1.\n",
    "- Detect and deal with any null values in the dataset.\n",
    "- For Age, replace null values with the median age for the dataset.\n",
    "- For Embarked, drop the rows that contain null values\n",
    "- One-Hot Encode categorical columns such as Embarked.\n",
    "- Store the target column, Survived, in a separate variable and remove it from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Sex = (df.Sex =='male').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age = df.Age.fillna(df.Age.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "df_Embarked_ohe = onehotencoder.fit_transform(np.array([df['Embarked'].values]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe = pd.concat([df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']],\n",
    "                    pd.DataFrame(df_Embarked_ohe.todense())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohe.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_ohe.Survived\n",
    "X = df_ohe.drop(columns =['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Training and Testing Sets\n",
    "\n",
    "Split data into training and testing sets. \n",
    "\n",
    "* Import `train_test_split` from the `sklearn.model_selection` module\n",
    "* Use `train_test_split` to split thr data into training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Data\n",
    "Normalize after splitting our data into training and testing sets. This is to avoid information \"leaking\" from our test set into our training set. Normalization (also sometimes called Standardization or Scaling) means making sure that all of data is represented at the same scale. The most common way to do this is to convert all numerical values to z-scores.\n",
    "\n",
    "Since KNN is a distance-based classifier, if data is in different scales, then larger scaled features have a larger impact on the distance between points.\n",
    "\n",
    "- Import and instantiate a StandardScaler object.\n",
    "- Use the scaler's .fit_transform() method to create a scaled version of our training dataset.\n",
    "- Use the scaler's .transform() method to create a scaled version of our testing dataset.\n",
    "- The result returned by the fit_transform and transform calls will be numpy arrays, not a pandas DataFrame. Create a new pandas DataFrame out of this object called scaled_df. To set the column names back to their original state, set the columns parameter to one_hot_df.columns.\n",
    "- Print out the head of scaled_df to ensure everything worked correctly.\n",
    "\n",
    "\n",
    "The scaler also scaled binary/one-hot encoded columns, too! But each binary column still only contains 2 values, meaning the overall information content of each column has not changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_df_train = scaler.fit_transform(X_train)\n",
    "scaled_df_test = scaler.transform(X_test)\n",
    "scaled_df_train = pd.DataFrame(scaled_df_train, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a KNN Model\n",
    "Time to train a KNN classifier and validate its accuracy.\n",
    "\n",
    "\n",
    "- Import KNeighborsClassifier from the sklearn.neighbors module.\n",
    "- Instantiate a classifier. For now, you can just use the default parameters.\n",
    "- Fit the classifier to the training data/labels\n",
    "- Use the classifier to generate predictions on the test data. Store these predictions inside the variable test_preds.\n",
    "\n",
    "\n",
    "Import all the necessary evaluation metrics from sklearn.metrics and complete the print_metrics() function so that it prints out Precision, Recall, Accuracy, and F1-Score when given a set of labels (the true values) and preds (the models predictions).\n",
    "\n",
    "Finally, use print_metrics() to print out the evaluation metrics for the test predictions stored in test_preds, and the corresponding labels in y_test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(scaled_df_train, y_train)\n",
    "test_preds = knn.predict(scaled_df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.782051282051282\n",
      "Recall Score: 0.7011494252873564\n",
      "Accuracy Score: 0.8063063063063063\n",
      "F1 Score: 0.7393939393939394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "def print_metrics(labels, preds):\n",
    "    print(f\"Precision Score: {precision_score(labels, preds)}\")\n",
    "    print(f\"Recall Score: {recall_score(labels, preds)}\")\n",
    "    print(f\"Accuracy Score: {accuracy_score(labels, preds)}\")\n",
    "    print(f\"F1 Score: {f1_score(labels, preds)}\")\n",
    "                        \n",
    "print_metrics(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model Performance\n",
    "Try to find the optimal number of neighbors to use for the classifier: Iterate over multiple values of k and find the value of k that returns the best overall performance.\n",
    "\n",
    "The skeleton function takes in six parameters:\n",
    "\n",
    "X_train\n",
    "y_train\n",
    "X_test\n",
    "y_test\n",
    "min_k (default is 1)\n",
    "max_k (default is 25)\n",
    "\n",
    "Create two variables, best_k and best_score\n",
    "- Iterate through every odd number between min_k and max_k + 1.\n",
    "For each iteration:\n",
    "- Create a new KNN classifier, and set the n_neighbors parameter to the current value for k, as determined by our loop.\n",
    "- Fit this classifier to the training data.\n",
    "- Generate predictions for X_test using the fitted classifier.\n",
    "- Calculate the F1-score for these predictions.\n",
    "- Compare this F1-score to best_score. If better, update best_score and best_k.\n",
    "- Once all iterations are complete, print out the best value for k and the F1-score it achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_k(X_train, y_train, X_test, y_test, min_k=1, max_k=25):\n",
    "    best_k = 0\n",
    "    best_score = 0.0\n",
    "    for k in range(min_k, max_k+1, 2):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        preds = knn.predict(X_test)\n",
    "        f1 = f1_score(y_test, preds)\n",
    "        if f1 > best_score:\n",
    "            best_k = k\n",
    "            best_score = f1\n",
    "    \n",
    "    print(f\"Best Value for k: {best_k}\")\n",
    "    print(f\"F1-Score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Value for k: 11\n",
      "F1-Score: 0.778443113772455\n"
     ]
    }
   ],
   "source": [
    "find_best_k(scaled_df_train, y_train, scaled_df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model performance has improved by over 4 percent by finding an optimal value for k. For further tuning, use scikit-learn's built in **Grid Search** to perform a similar exhaustive check of hyper-parameter combinations and fine tune model performance. [sklearn documentation !](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\\n\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
